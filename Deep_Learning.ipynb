{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_Jp0pVMV6qw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize\n",
    "import os\n",
    "import h5py\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "# Keras imports\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential, model_from_json, Model\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, Activation, Dropout, Flatten, Dense, Input\n",
    "from keras.layers import Input, Conv2D, UpSampling2D, BatchNormalization, Activation, add, concatenate,GlobalAveragePooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "87z03EpCFbXV",
    "outputId": "18d4cb71-5a94-4390-93fb-0c4ec1a1c235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gd_qEbYVIE5c",
    "outputId": "6186a683-1c57-4871-8d1b-05e00cb2a8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Colab Notebooks\n"
     ]
    }
   ],
   "source": [
    "cd /content/gdrive/My Drive/Colab Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copy images to folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = './images/im/'\n",
    "train_dest = './images/train'\n",
    "valid_dest = './images/val'\n",
    "sub_dest = './images/submission'\n",
    "def copy_valid(y,X,src,dest):\n",
    "    for i in range(600, X.shape[0]):\n",
    "        image = X[i]\n",
    "        if (y[i]==0):\n",
    "            destination = dest + '/benign'\n",
    "        else:\n",
    "            destination = dest + '/malignant'\n",
    "        file = src + str(image) + '.jpg'\n",
    "        shutil.copy(file,destination)\n",
    "def copy_train(y,X,src,dest):\n",
    "    for i in range(0, 600):\n",
    "        image = X[i]\n",
    "        if (y[i]==0):\n",
    "            destination = dest + '/benign'\n",
    "        else:\n",
    "            destination = dest + '/malignant'\n",
    "        file = src + str(image) + '.jpg'\n",
    "        shutil.copy(file,destination)\n",
    "def copy_submission(y,X,src,dest):\n",
    "    for i in range(0, X.shape[0]):\n",
    "        image = X[i]\n",
    "        if (y[i]==0):\n",
    "            destination = dest + '/benign'\n",
    "        else:\n",
    "            destination = dest + '/malignant'\n",
    "        file = src + str(image) + '.jpg'\n",
    "        shutil.copy(file,destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv('data/test.csv')\n",
    "df = pd.read_csv('data/train.csv')\n",
    "X_df = df['ImageId']\n",
    "y_df = df['Malignant']\n",
    "X = X_df.values\n",
    "y = y_df.values\n",
    "\n",
    "# copy images to train, val and submission folders\n",
    "copy_submission(y,df_submission['ImageId'].values,src,sub_dest)\n",
    "copy_train(y,X,src,train_dest)\n",
    "copy_valid(y,X,src,valid_dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OK7KMgbFFUVp"
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zge00sa3WDxf"
   },
   "outputs": [],
   "source": [
    "# paths to save results\n",
    "model_name = \"melanoma_detection\"\n",
    "model_path = './models/models_trained/' +model_name+'/'\n",
    "\n",
    "# paths to training and testing data\n",
    "train_data_dir = './images/train'\n",
    "validation_data_dir = './images/val'\n",
    "submission_data_dir = './images/submission'\n",
    "\n",
    "# paths to weight files\n",
    "top_model_weights_path = './vgg-05-0.54.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJLD2i_hWD6S"
   },
   "outputs": [],
   "source": [
    "### other hyperparameters\n",
    "nb_train_samples = 600\t\t\t\t# Training samples\n",
    "nb_train_samples_benign = 354\t\t# Testing samples\n",
    "nb_train_samples_malignant = 246\t# Malignant Training samples\n",
    "nb_validation_samples = 100\t\t\t# Malignant Training samples\n",
    "nb_validation_samples_benign = 64\t#Benign Training samples\n",
    "nb_validation_samples_maligant = 36\t# Malignant Testing samples\n",
    "nb_epoch = 50\n",
    "img_width, img_height = 224, 224\n",
    "class_weights={0:1,1:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LSn3rh7qF6XO"
   },
   "source": [
    "# checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h_G8fjHMF8Ge"
   },
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "checkpoint_path=\"./vgg-{epoch:02d}-{val_matthews_correlation:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "save_best_only = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "78zR4f5tGDpL"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43n9p26-WECn"
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "def load_data(data_type):\n",
    "    \n",
    "    \n",
    "    print('Loading data: ', data_type)\n",
    "    if data_type == 'train':\n",
    "        data_dir = train_data_dir\n",
    "        print('Loading train data... ')\n",
    "    elif data_type == 'submission':\n",
    "        data_dir = submission_data_dir\n",
    "        print('Loading submission data... ')\n",
    "    else:\n",
    "        data_dir = validation_data_dir\n",
    "        print('Loading test data... ')\n",
    "\n",
    "    \n",
    "    malignant_path = os.path.join(data_dir, 'malignant')\n",
    "    malignant_list = os.listdir(malignant_path)  # get a list of all malignant image files in directory\n",
    "    malignant_num = len(malignant_list)\n",
    "    benign_path = os.path.join(data_dir, 'benign')\n",
    "    benign_list = os.listdir(benign_path)\n",
    "    benign_num = len(benign_list)\n",
    "\n",
    "    _X = np.empty((benign_num + malignant_num,img_height,img_width,3), dtype='float32')\n",
    "    _y = np.zeros((benign_num + malignant_num, ), dtype='uint8')\n",
    "\n",
    "    # store the malignant\n",
    "    for i, malignant_file in enumerate(malignant_list):\n",
    "        img = image.load_img(os.path.join(malignant_path, malignant_file), grayscale=False, target_size=(img_height,img_width))\n",
    "        _X[i] = image.img_to_array(img)\n",
    "        \n",
    "    for i, benign_file in enumerate(benign_list):\n",
    "        img = image.load_img(os.path.join(benign_path, benign_file), grayscale=False, target_size=(img_height,img_width))\n",
    "        _X[i + malignant_num] = image.img_to_array(img)\n",
    "        _y[i + malignant_num] = 1\n",
    "    return _X, _y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x7OJNmL1GGq5"
   },
   "outputs": [],
   "source": [
    "# Build matrix using name list\n",
    "def build_matrix(X,folder):\n",
    "    # load images\n",
    "    M = []\n",
    "    for i in range(0, X.shape[0]):\n",
    "        image = X[i]\n",
    "        im = plt.imread(folder + '/im/'  + str(image) + '.jpg')/255\n",
    "        mask = plt.imread(folder + '/im/'  + str(image) + '_segmentation.jpg')/255 \n",
    "        img = im* np.stack([mask,mask,mask],axis=-1)\n",
    "        img_down = resize(img,(224,224), mode='reflect',anti_aliasing = True) \n",
    "        M.append(img_down)\n",
    "    return np.asarray(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VeLVwUyKGGt4"
   },
   "outputs": [],
   "source": [
    "# MEAN 0 STD 1\n",
    "def standardize(img):\n",
    "    mean = np.mean(img)\n",
    "    std = np.std(img)\n",
    "    img = (img - mean) / std\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SUN-HWYoGv_-"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAyCFutz7FVB"
   },
   "outputs": [],
   "source": [
    "def matthews_correlation(y_true, y_pred):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "  \n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    return (true_positives / (predicted_positives + K.epsilon()))\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return (true_positives / (possible_positives + K.epsilon()))\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    \n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return ( 2*(precision * recall) / (precision + recall + K.epsilon()) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rtjgFz5jPfXj"
   },
   "source": [
    "# DRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URs4HyaYVgnm"
   },
   "outputs": [],
   "source": [
    "# Residual block\n",
    "def res_block(x,size):\n",
    "    \n",
    "    reduce = (int)(size/4)\n",
    "    res_path = Conv2D(filters= reduce , kernel_size=(1, 1),strides = (2,2), padding='same')(x)\n",
    "    res_path = BatchNormalization()(res_path)\n",
    "    res_path = Activation(activation='relu')(res_path)\n",
    "    \n",
    "    res_path = Conv2D(filters= reduce , kernel_size=(3, 3), padding='same')(res_path)\n",
    "    res_path = BatchNormalization()(res_path)\n",
    "    res_path = Activation(activation='relu')(res_path)\n",
    "    \n",
    "    res_path = Conv2D(filters= size , kernel_size=(1, 1), padding='same')(x)\n",
    "    res_path = BatchNormalization()(res_path)\n",
    "    \n",
    "    res_path = concatenate([x, res_path])\n",
    "    res_path = Activation(activation='relu')(res_path)\n",
    "    \n",
    "       \n",
    "    return res_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvMMa5Q7VgqP"
   },
   "outputs": [],
   "source": [
    "# Deep Residual Network training\n",
    "def DRN():\n",
    "    \n",
    "    img_input = Input(shape= (224, 224, 3))\n",
    "    \n",
    "    x = Conv2D(64, (7, 7), padding='same',strides = (2,2), name='conv1')(img_input)\n",
    "    x = MaxPooling2D(pool_size = (3,3), strides = (2,2))(x)\n",
    "    x = res_block(x,256)\n",
    "    x = res_block(x,512)\n",
    "    x = res_block(x,1024)\n",
    "    x = res_block(x,2048)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024,activation='relu')(x) #dense layer 2\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(512,activation='relu')(x) #dense layer 2\n",
    "    x = Dense(512,activation='relu')(x) #dense layer 2\n",
    "    output = Dense(units = 2, activation = 'softmax')(x)\n",
    "    model = Model(inputs=img_input, outputs=output)\n",
    "    #model.load_weights(top_model_weights_path)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "                  metrics=['accuracy',matthews_correlation])\n",
    "\n",
    "    # Data augmentation for training images\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range=270,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,samplewise_center = True, samplewise_std_normalization = True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "    # Data augmentation for validation images\n",
    "    valid_datagen = ImageDataGenerator(\n",
    "        rotation_range=270,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,samplewise_center = True, samplewise_std_normalization = True,\n",
    "        fill_mode='nearest')       \n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height,img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    validation_generator = valid_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_height,img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "   \n",
    "    scores = model.fit_generator(\n",
    "                    train_generator,\n",
    "                    samples_per_epoch=nb_train_samples,\n",
    "                    nb_epoch=30,verbose = 1,\n",
    "                    validation_data=validation_generator,\n",
    "                    nb_val_samples=nb_validation_samples,\n",
    "                    class_weight=class_weights,\n",
    "                    callbacks = save_best_only)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMZceT71G4yS"
   },
   "source": [
    "## Train top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qkzF6JxTWEeY"
   },
   "outputs": [],
   "source": [
    "def train_top_model():\n",
    "    \n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    # Pre-build model\n",
    "    vg = VGG16(include_top = False, weights = 'imagenet', input_shape = (224, 224, 3))\n",
    "    for layer in vg.layers[:]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model  = Sequential()\n",
    "    model.add(vg)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    #model.load_weights(top_model_weights_path)\n",
    "    \n",
    "    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy',matthews_correlation])\n",
    "    \n",
    "    \n",
    "    # Data augmentation for training images\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range=270,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,samplewise_center = True, samplewise_std_normalization = True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "    # Data augmentation for validation images\n",
    "    valid_datagen = ImageDataGenerator(\n",
    "        rotation_range=270,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,samplewise_center = True, samplewise_std_normalization = True,\n",
    "        fill_mode='nearest')       \n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height,img_width),\n",
    "        batch_size=16,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    validation_generator = valid_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_height,img_width),\n",
    "        batch_size=16,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    scores = model.fit_generator(\n",
    "                    train_generator,\n",
    "                    samples_per_epoch=nb_train_samples,\n",
    "                    nb_epoch=30,verbose = 1,\n",
    "                    validation_data=validation_generator,\n",
    "                    nb_val_samples=nb_validation_samples,\n",
    "                    class_weight=class_weights,\n",
    "                    callbacks = save_best_only)\n",
    "\n",
    "\n",
    "    model.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Bs63zEqHACS"
   },
   "source": [
    "## Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qyFd0eGtWEmq"
   },
   "outputs": [],
   "source": [
    "def fine_tune():\n",
    "\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "  \n",
    "    # Pre-build model\n",
    "    vg = VGG16(include_top = False, weights = 'imagenet', input_shape = (224, 224, 3))\n",
    "    for layer in vg.layers[:]:\n",
    "        layer.trainable = True\n",
    "    model  = Sequential()\n",
    "    model.add(vg)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    #model.load_weights(top_model_weights_path)\n",
    "\n",
    "    # compile the model with a SGD/momentum optimizer\n",
    "    # and a very slow learning rate.\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=1e-6, momentum=0.9),\n",
    "                  metrics=['accuracy',matthews_correlation])\n",
    "\n",
    "    # Data augmentation for training images\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range=270,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,samplewise_center = True, samplewise_std_normalization = True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "    # Data augmentation for validation images\n",
    "    valid_datagen = ImageDataGenerator(\n",
    "        rotation_range=270,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,samplewise_center = True, samplewise_std_normalization = True,\n",
    "        fill_mode='nearest')       \n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height,img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    validation_generator = valid_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_height,img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    # fine-tune the model\n",
    "    scores = model.fit_generator(\n",
    "                    train_generator,\n",
    "                    samples_per_epoch=nb_train_samples,\n",
    "                    nb_epoch=50,verbose = 1,\n",
    "                    validation_data=validation_generator,\n",
    "                    nb_val_samples=nb_validation_samples,\n",
    "                    class_weight=class_weights,\n",
    "                    callbacks = save_best_only)\n",
    "\n",
    "    # save the model\n",
    "    json_string = model.to_json()\n",
    "\n",
    "    with open('final_model_architecture.json', 'w') as f:\n",
    "        f.write(json_string)\n",
    "\n",
    "    model.save_weights('final_weights.h5')\n",
    "\n",
    "    return model, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ms9koc9SHFpg"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHGUx7R9WEwK"
   },
   "outputs": [],
   "source": [
    "# Make prediction for submission data given a model\n",
    "def prediction(model):\n",
    "    #Load data as a Numpy array\n",
    "    df_submission = pd.read_csv('data/test.csv')\n",
    "\n",
    "    X_test = build_matrix(df_submission['ImageId'].values,folder='data')\n",
    "    \n",
    "    y_pred = model.predict_classes(X_test, batch_size=64)\n",
    "    np.savetxt('y_pred.txt', y_pred)\n",
    "\n",
    "    y_score = model.predict_proba(X_test, batch_size=64)\n",
    "    np.savetxt('y_score.txt', y_score)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jC-OEeTfioHj"
   },
   "outputs": [],
   "source": [
    "# load model from hdf5 file\n",
    "def load_model():\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    \n",
    "    # Pre-build model\n",
    "    vg = VGG16(include_top = False, weights = 'imagenet', input_shape = (224, 224, 3))\n",
    "    model  = Sequential()\n",
    "    model.add(vg)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "    model.load_weights(top_model_weights_path)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=1e-5, momentum=0.9),\n",
    "                  metrics=['accuracy',matthews_correlation])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stCt5oYwH0zd"
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVTG30nyH2iQ"
   },
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    # Retrieve a list of accuracy results on training and test data\n",
    "    # sets for each training epoch\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "\n",
    "    # Retrieve a list of list results on training and test data\n",
    "    # sets for each training epoch\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Get number of epochs\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    # Plot training and validation accuracy per epoch\n",
    "    plt.plot(epochs, acc, label = \"training\")\n",
    "    plt.plot(epochs, val_acc, label = \"validation\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.title('Training and validation accuracy')\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # Plot training and validation loss per epoch\n",
    "    plt.plot(epochs, loss, label = \"training\")\n",
    "    plt.plot(epochs, val_loss, label = \"validation\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uUfjo7MyH2lj"
   },
   "outputs": [],
   "source": [
    "def make_plots(h):\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(h.history['loss'], 'r-')\n",
    "    plt.plot(h.history['val_loss'], 'b-')\n",
    "    plt.title('Loss plot')\n",
    "    plt.legend(['Training loss', 'Validation loss'])\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(h.history['f1'], 'r-')\n",
    "    plt.plot(h.history['val_f1'], 'b-')\n",
    "    plt.title('F1 plot')\n",
    "    plt.legend(['Training F1', 'Validation F1'])\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(h.history['precision'], 'r-')\n",
    "    plt.plot(h.history['val_precision'], 'b-')\n",
    "    plt.title('Precision plot')\n",
    "    plt.legend(['Training precision', 'Validation precision'])\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(h.history['recall'], 'r-')\n",
    "    plt.plot(h.history['val_recall'], 'b-')\n",
    "    plt.title('Recall plot')\n",
    "    plt.legend(['Training recall', 'Validation recall'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jEEFZ12zHJLL"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xVYVqdPXjB7"
   },
   "outputs": [],
   "source": [
    "### train top model and save weights\n",
    "train_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9srKdeiBWzun"
   },
   "outputs": [],
   "source": [
    "### Train Vgg network using top model weights\n",
    "model,history = fine_tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlwauZclmBJF"
   },
   "outputs": [],
   "source": [
    "#Load submission data as a Numpy array\n",
    "df_submission = pd.read_csv('data/test.csv')\n",
    "X_test = build_matrix(df_submission['ImageId'].values,folder='data')\n",
    "sub = standardize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcV8HjCEjF0L"
   },
   "outputs": [],
   "source": [
    "# model = load_model()\n",
    "# y = prediction(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EHktOdLMjMP6"
   },
   "outputs": [],
   "source": [
    "### make predictions\n",
    "y_pred = model.predict(sub)\n",
    "y = np.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "knU2a41jWE47"
   },
   "outputs": [],
   "source": [
    "### submission for kaggle\n",
    "i=0\n",
    "for Id in df_submission['ImageId']:\n",
    "    value = y[i] \n",
    "    df_submission.loc[df_submission['ImageId'] == Id, 'Malignant'] = value\n",
    "    i=i+1\n",
    "    \n",
    "df_submission['Malignant'] = df_submission['Malignant'].astype(int) # This line is mandatory to be sure to have integer\n",
    "print(df_submission.head(3))\n",
    "df_submission.to_csv('data/1106.csv', index=None, sep=',', mode='w')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Learning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
